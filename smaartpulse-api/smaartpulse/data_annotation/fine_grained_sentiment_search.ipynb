{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from gensim import utils\n",
    "from utils import check_super_phrase, dict_of_phrases\n",
    "import math\n",
    "\n",
    "# This is the pre-specified window size.\n",
    "window = 4 * 4\n",
    "REVIEW_FiELD = 9\n",
    "\n",
    "# This is the directory which contains the curated phrases files.\n",
    "#PHRASE_DIR = \"curated_aspects_2\"\n",
    "PHRASE_DIR = \"fine-grained-phrases\"\n",
    "\n",
    "# REVIEWS_FILE contains 1 sentence in every line\n",
    "REVIEWS_FILE = \"temp.txt\"\n",
    "OUTPUT_FILE = \"temp_output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance is not NEG_up_to_the_mark\n",
      "back_camera very very not_good\n",
      "with this new phone i have yet NEG_to NEG_have NEG_any NEG_errors NEG_or NEG_crashes\n",
      "i have had no_lag and heating_problem with this device till now\n",
      "the_display_is not NEG_better\n",
      "imaging youd be unhappy_with\n",
      "apps and games_are smooth without NEG_a_lag\n",
      "cannot NEG_open NEG_it NEG_to NEG_replace_the_battery\n",
      "battery isnt NEG_that NEG_great\n",
      "battery isnt NEG_awesome\n",
      "volume can NEG_be NEG_better\n",
      "NEG_everything NEG_is_good NEG_except processor\n",
      "i have yet NEG_to NEG_have NEG_any NEG_issues NEG_or NEG_crashes\n"
     ]
    }
   ],
   "source": [
    "# The code searches for the brand names, aspect names and sentiment type in products\n",
    "\n",
    "# reading the phrases from the phrase_dir in hierarchical way\n",
    "phrase_dict, global_dict = dict_of_phrases(PHRASE_DIR)\n",
    "\n",
    "# From a review extracting all the valid phrases and do a super phrase\n",
    "# check on them\n",
    "# Opening the reviews file and output file\n",
    "infile = utils.smart_open(REVIEWS_FILE, 'rb')\n",
    "outfile = utils.smart_open(OUTPUT_FILE, 'wb')\n",
    "\n",
    "line_num = 0\n",
    "for line in infile:\n",
    "    line_num += 1\n",
    "    sentence = utils.to_unicode(line).split(\"~\")[REVIEW_FiELD]\n",
    "\n",
    "    # By doing this words like don't and can't will become dont and cant\n",
    "    sentence = sentence.replace(\"'\", \"\")\n",
    "\n",
    "    # Replacing dashes with spaces so as to prevent errors\n",
    "    # in detecting word boundaries\n",
    "    sentence = sentence.replace(\"-\", \" \")\n",
    "\n",
    "    # Making the sentence lowercase\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    toks = sentence.split(\" \")\n",
    "    toks1 = [word.strip() for word in toks]\n",
    "\n",
    "    temp_dict = dict()\n",
    "    \n",
    "    if len(toks1) > 1:\n",
    "        # what is the importance of this condition ?\n",
    "        if line_num % 1000 is 0:\n",
    "            print 'Processed %i lines' % line_num\n",
    "\n",
    "        # Modifying the code to incorporate unigrams\n",
    "        for i, start_token in enumerate(toks1[0:]):\n",
    "            \n",
    "            # starting the range from zero to incorporate unigrams\n",
    "            for j in range(0, window):\n",
    "                if i + j + 1 <= len(toks1):\n",
    "                    new_string = \" \".join(toks1[i:i + j + 1])\n",
    "                    if new_string in global_dict:\n",
    "                        temp_dict[new_string] = global_dict[new_string]\n",
    "\n",
    "    # Keeping only superphrases                    \n",
    "    line_phrase_dict = check_super_phrase(temp_dict.keys(), temp_dict, window)\n",
    "\n",
    "    # sorting the phrases in descending order of coherence\n",
    "    temp_list = sorted(line_phrase_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    string = \" \".join(toks1)\n",
    "\n",
    "    line_phrase_list = list()\n",
    "    # filtering out the non-overlapping phrases in line and adding unigrams in line_phrase_list\n",
    "    for phrase, _ in temp_list:\n",
    "        if len(phrase.split(\" \")) is 1:\n",
    "            line_phrase_list.append(phrase)\n",
    "        else:\n",
    "            string = re.sub(r'\\b' + phrase + r'\\b', \"_\".join(phrase.split()), string)\n",
    "\n",
    "    # Tokenize the string by whitespace. SEARCH the tokens for underscores.\n",
    "    # if \"_\" is found, REPLACE the \"_\" by \" \", to get the desired phrase.\n",
    "    toks = string.split(\" \")\n",
    "    for word in toks:\n",
    "        if \"_\" in word:\n",
    "            word = word.replace(\"_\", \" \")\n",
    "            line_phrase_list.append(word)\n",
    "\n",
    "    print >> outfile, utils.to_utf8(line.strip()), '~',\n",
    "\n",
    "    # Detecting the negation in a sentence\n",
    "    transformed = re.sub(\n",
    "        r'\\b(?:not|cannot|could\\ be|can\\ be|yet\\ to|never|no|nothing|nowhere|noone|none|havent|hasnt|hadnt|cant|couldnt|without|shouldnt|wont|wouldnt|dont|doesnt|dosnt|didnt|isnt|arent|aintnever|werent)\\b[\\w\\s_]+',\n",
    "        lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0), count=7), string)\n",
    "\n",
    "    # Detecting second kind of negation in sentence\n",
    "    transformed2 = re.sub(\n",
    "        r'[\\w\\s_]+\\b(?:except)\\b',\n",
    "        lambda match: re.sub(r'(\\s*)(\\w+)', r'\\1NEG_\\2', match.group(0), count=5), transformed)\n",
    "    \n",
    "    print transformed2\n",
    "    \n",
    "    transformed_toks = transformed2.split(\" \")\n",
    "\n",
    "    # Search for the corresponding phrase in the aspect and sentiment category\n",
    "    valid_tuples = list()\n",
    "    \n",
    "    # line_phrase_list contains the tags identified for the review\n",
    "    for tag in line_phrase_list:\n",
    "        for category in phrase_dict:\n",
    "            tag_underscore = tag.replace(\" \", \"_\")\n",
    "            tag_new = tag_underscore\n",
    "\n",
    "            # If we detect negations in transformed tokens, \n",
    "            # then include \"NEG_\" string at the start of the corresponding phrase \n",
    "            if \"sentiments\" == category or \"aspects-sentiments\" == category:\n",
    "                if 'NEG_' + tag_underscore in transformed_toks:\n",
    "                    tag_new = 'NEG_' + tag_underscore\n",
    "            \n",
    "            for cat_type in phrase_dict[category]:\n",
    "                if tag in phrase_dict[category][cat_type]:\n",
    "\n",
    "                    # Dealing with the case of \"aspects-sentiments\" directory\n",
    "                    if category == \"aspects-sentiments\":\n",
    "                        aspect_type = cat_type.split(\"_\")[0]\n",
    "                        sent_type = cat_type.split(\"_\")[1]\n",
    "\n",
    "                        # Changing the polarity of the sentiment due to negation detection\n",
    "                        if tag_new == 'NEG_' + tag_underscore:\n",
    "                            if sent_type == \"positive\":\n",
    "                                sent_type = \"negative\"\n",
    "                            elif sent_type == \"negative\":\n",
    "                                sent_type = \"positive\"\n",
    "                            elif sent_type == \"most-positive\":\n",
    "                                sent_type = \"most-negative\"\n",
    "                            elif sent_type == \"most-negative\":\n",
    "                                sent_type = \"most-positive\"\n",
    "                            elif sent_type == \"neutral\":\n",
    "                                sent_type = \"negative\"\n",
    "\n",
    "                        aspect_tuple = (category.split(\"-\")[0], aspect_type, tag_new)\n",
    "                        sent_tuple = (category.split(\"-\")[1], sent_type, tag_new)\n",
    "                        print >> outfile, (aspect_tuple, sent_tuple), \"~\",\n",
    "        \n",
    "                    # For \"aspects\" and \"sentiments\" class of keywords\n",
    "                    else:\n",
    "                        tag = tag.replace(\" \", \"_\")\n",
    "\n",
    "                        try:\n",
    "                            tag_index = toks.index(tag)\n",
    "                        except ValueError:\n",
    "                            print \"line is\"\n",
    "\n",
    "                        if tag_new == 'NEG_' + tag_underscore:\n",
    "                            if cat_type == \"positive\":\n",
    "                                cat_type = \"negative\"\n",
    "                            elif cat_type == \"negative\":\n",
    "                                cat_type = \"positive\"\n",
    "                            elif cat_type == \"most-positive\":\n",
    "                                cat_type = \"most-negative\"\n",
    "                            elif cat_type == \"most-negative\":\n",
    "                                cat_type = \"most-positive\"\n",
    "                            elif cat_type == \"neutral\":\n",
    "                                cat_type = \"negative\"\n",
    "\n",
    "                        valid_tuples.append((category, cat_type, str(tag_new), tag_index))\n",
    "\n",
    "    # find the closest sentiment to an aspect\n",
    "    sorted_aspect_sent_list = sorted(valid_tuples, key=lambda x: x[3])\n",
    "    \n",
    "    for k, aspect_tuple in enumerate(sorted_aspect_sent_list):\n",
    "        if aspect_tuple[0] == 'aspects':\n",
    "            sent_tuple = min(sorted_aspect_sent_list,\n",
    "                             key=lambda x: abs(x[3] - aspect_tuple[3]) if (x[0] == 'sentiments') else 1000)\n",
    "            if sent_tuple[0] == \"sentiments\":\n",
    "                if math.fabs(aspect_tuple[-1] - sent_tuple[-1]) < 15:\n",
    "                    print >> outfile, (aspect_tuple[0:-1], sent_tuple[0:-1]), \"~\",\n",
    "\n",
    "    print >> outfile, '\\n',\n",
    "\n",
    "infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "#pprint.pprint(phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
